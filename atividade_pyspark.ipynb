{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abono_pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1> AULA13 - PYSPARK - ATIVIDADE ABONO <h1>\n",
        "<ul>\n",
        "  <li> Instalar e importar as bibliotecas do spark e pyspark </li>\n",
        "  <li> Criar um spark context com o app name = “Exercicio Pyspark” </li>\n",
        "  <li> Subir o arquivo “pedido detalhe” </li>\n",
        "  <li> Criar um dataframe lendo o arquivo pedido_detalhe.csv </li>\n",
        "  <li> Criar uma nova coluna chamada “data_atual” com a data atual </li>\n",
        "  <li> Converter a coluna data_atual para o tipo data. </li>\n",
        "  <li> Criar uma nova coluna com a  multiplicação da quantidade e valor chamada “total” </li>\n",
        "  <li> gravar no formato parquet apenas os pedidos com a VALOR maior que 100 </li>\n",
        "\n",
        "<ul>"
      ],
      "metadata": {
        "id": "927dMTgoi-v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando as bibliotecas\n",
        "%%sh\n",
        "sudo pip install spark\n",
        "sudo pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "873GOVLujbWK",
        "outputId": "6fb9bb39-8b64-4920-f609-e0e6461220f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spark\n",
            "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
            "Building wheels for collected packages: spark\n",
            "  Building wheel for spark (setup.py): started\n",
            "  Building wheel for spark (setup.py): finished with status 'done'\n",
            "  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58762 sha256=95c16f538f9977db9e5c639803490a791de7a92333127ec1f3571118806c74d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/0e/f1/164619f9920fb447d294afaae11a7715bd442ded7225953d72\n",
            "Successfully built spark\n",
            "Installing collected packages: spark\n",
            "Successfully installed spark-0.2.1\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "Collecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py): started\n",
            "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=d4ac6da790a8e0e4a2fe1bfd085d0fb81abdad549c62c597eeac28c316f74d6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Importando as bibliotecas </h3>"
      ],
      "metadata": {
        "id": "WCdt7srRjqBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas, podemos transformar um dataframe do pandas em um dataframe do spark e o contrário também\n",
        "import pandas as pd\n",
        "\n",
        "# Importando o spark e o pyspark\n",
        "import spark,pyspark\n",
        "\n",
        "# Importando as bibliotecas do pyspark.sql \n",
        "from pyspark.sql import *\n",
        "\n",
        "# Importando as funções sql do spark\n",
        "# documentação https://spark.apache.org/docs/latest/api/sql/index.html\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "# Importando os tipos de dados do spark\n",
        "# Documentação https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
        "from pyspark.sql import types as t \n",
        "\n",
        "# Biblioteca datetime\n",
        "from datetime import datetime, date"
      ],
      "metadata": {
        "id": "e5Jcoul2juMV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important classes of Spark SQL and DataFrames:\n",
        "\n",
        "1. <b>pyspark.sql.SparkSession</b> -> Ponto principal de entrada para criar dataframes e  funcionalidades do SQL.\n",
        "2. <b>pyspark.sql.DataFrame</b> -> Coleção de dados distribuidas e agrupadas em colunas (tabela).\n",
        "3. <b>pyspark.sql.Column</b> -> Coluna do dataframe\n",
        "4. <b>pyspark.sql.Row linha</b> -> do dataframe\n",
        "5. <b>pyspark.sql.functions</b> -> Lista de funções embutidas para dataframes.\n",
        "6. <b>pyspark.sql.types</b> -> Lista de tipos de dados para as colunas do dataframe."
      ],
      "metadata": {
        "id": "3nAmAzP6kBzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 1 - Criar um spark context com o app name = “Exercicio Pyspark” </h3>"
      ],
      "metadata": {
        "id": "Y-K9uIpakdZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando uma Sessão do Spark (Spark Session)\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Exercicio Pyspark\").getOrCreate()"
      ],
      "metadata": {
        "id": "6mk4_HyDkib9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 2 - Criar um dataframe lendo o arquivo pedido_detalhe.csv </h3>"
      ],
      "metadata": {
        "id": "FYPg56lWkrTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertendo o dataframe do pandas para o do spark\n",
        "df_pandas = pd.read_csv(\"/content/pedido_detalhe.csv\", delimiter=\";\")\n",
        "df_spark = spark.createDataFrame(df_pandas)\n",
        "df_spark = df_spark.drop('Unnamed: 5', 'Unnamed: 6')\n",
        "df_spark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "909vkYY7kxCS",
        "outputId": "ac2659cc-976e-4a28-a7af-ebaa18b3ccb4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+----------+----------+-----+\n",
            "|id_venda_detalhe|id_pedido|id_produto|quantidade|valor|\n",
            "+----------------+---------+----------+----------+-----+\n",
            "|               1|        1|         1|         1|   60|\n",
            "|               2|        1|         2|         2|  200|\n",
            "|               3|        1|         3|         3|  400|\n",
            "|               4|        1|         4|         3|  400|\n",
            "|               5|        1|         5|         1|  350|\n",
            "|               6|        1|         6|         2|  800|\n",
            "|               7|        2|         1|         1|   60|\n",
            "|               8|        2|         2|         2|  200|\n",
            "|               9|        2|         3|         3|  400|\n",
            "|              10|        2|         4|         3|  400|\n",
            "|              11|        2|         5|         1|  350|\n",
            "|              12|        3|         6|         2|  800|\n",
            "|              13|        3|         1|         1|   60|\n",
            "|              14|        3|         2|         2|  200|\n",
            "|              15|        4|         3|         3|  400|\n",
            "|              16|        4|         4|         3|  400|\n",
            "|              17|        5|         5|         1|  350|\n",
            "|              18|        6|         6|         2|  800|\n",
            "|              19|        7|         1|         1|   60|\n",
            "|              20|        8|         2|         2|  200|\n",
            "+----------------+---------+----------+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 3 - Criar uma nova coluna chamada “data_atual” com a data atual </h3>"
      ],
      "metadata": {
        "id": "ShqFxB2QlxFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types as t\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "data_atual = date.today()\n",
        "\n",
        "#Criando novas colunas\n",
        "df_spark = df_spark.withColumn(\"data_atual\",f.lit(data_atual))\n",
        "df_spark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JQlhUuTlzyn",
        "outputId": "3adc0f2d-63aa-4e82-e22f-3c01afc99d52"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+----------+----------+-----+----------+\n",
            "|id_venda_detalhe|id_pedido|id_produto|quantidade|valor|data_atual|\n",
            "+----------------+---------+----------+----------+-----+----------+\n",
            "|               1|        1|         1|         1|   60|2022-05-12|\n",
            "|               2|        1|         2|         2|  200|2022-05-12|\n",
            "|               3|        1|         3|         3|  400|2022-05-12|\n",
            "|               4|        1|         4|         3|  400|2022-05-12|\n",
            "|               5|        1|         5|         1|  350|2022-05-12|\n",
            "|               6|        1|         6|         2|  800|2022-05-12|\n",
            "|               7|        2|         1|         1|   60|2022-05-12|\n",
            "|               8|        2|         2|         2|  200|2022-05-12|\n",
            "|               9|        2|         3|         3|  400|2022-05-12|\n",
            "|              10|        2|         4|         3|  400|2022-05-12|\n",
            "|              11|        2|         5|         1|  350|2022-05-12|\n",
            "|              12|        3|         6|         2|  800|2022-05-12|\n",
            "|              13|        3|         1|         1|   60|2022-05-12|\n",
            "|              14|        3|         2|         2|  200|2022-05-12|\n",
            "|              15|        4|         3|         3|  400|2022-05-12|\n",
            "|              16|        4|         4|         3|  400|2022-05-12|\n",
            "|              17|        5|         5|         1|  350|2022-05-12|\n",
            "|              18|        6|         6|         2|  800|2022-05-12|\n",
            "|              19|        7|         1|         1|   60|2022-05-12|\n",
            "|              20|        8|         2|         2|  200|2022-05-12|\n",
            "+----------------+---------+----------+----------+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 4 - Converter a coluna data_atual para o tipo data.</h3><br>"
      ],
      "metadata": {
        "id": "E8pQwUsiobI8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGGta0psb4aR",
        "outputId": "c9bff69f-6dad-4f69-f7b7-e9f7db270b2b"
      },
      "source": [
        "# Criando uma coluna e convertendo para date\n",
        "df_spark.withColumn(\"data_atual\",df_spark.data_atual.cast(t.DateType()))\n",
        "df_spark.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+----------+----------+-----+----------+\n",
            "|id_venda_detalhe|id_pedido|id_produto|quantidade|valor|data_atual|\n",
            "+----------------+---------+----------+----------+-----+----------+\n",
            "|               1|        1|         1|         1|   60|2022-05-12|\n",
            "|               2|        1|         2|         2|  200|2022-05-12|\n",
            "|               3|        1|         3|         3|  400|2022-05-12|\n",
            "|               4|        1|         4|         3|  400|2022-05-12|\n",
            "|               5|        1|         5|         1|  350|2022-05-12|\n",
            "|               6|        1|         6|         2|  800|2022-05-12|\n",
            "|               7|        2|         1|         1|   60|2022-05-12|\n",
            "|               8|        2|         2|         2|  200|2022-05-12|\n",
            "|               9|        2|         3|         3|  400|2022-05-12|\n",
            "|              10|        2|         4|         3|  400|2022-05-12|\n",
            "|              11|        2|         5|         1|  350|2022-05-12|\n",
            "|              12|        3|         6|         2|  800|2022-05-12|\n",
            "|              13|        3|         1|         1|   60|2022-05-12|\n",
            "|              14|        3|         2|         2|  200|2022-05-12|\n",
            "|              15|        4|         3|         3|  400|2022-05-12|\n",
            "|              16|        4|         4|         3|  400|2022-05-12|\n",
            "|              17|        5|         5|         1|  350|2022-05-12|\n",
            "|              18|        6|         6|         2|  800|2022-05-12|\n",
            "|              19|        7|         1|         1|   60|2022-05-12|\n",
            "|              20|        8|         2|         2|  200|2022-05-12|\n",
            "+----------------+---------+----------+----------+-----+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 5 - Criar uma nova coluna com a multiplicação da quantidade e valor chamada “total” </h3>"
      ],
      "metadata": {
        "id": "wKIPv39aqX7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_spark = df_spark.withColumn(\"total\",df_spark.quantidade * df_spark.valor)\n",
        "df_spark.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL6cS19MqjLR",
        "outputId": "c50aea0c-1ca4-4e0b-f432-e99b666000f9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+----------+----------+-----+----------+-----+\n",
            "|id_venda_detalhe|id_pedido|id_produto|quantidade|valor|data_atual|total|\n",
            "+----------------+---------+----------+----------+-----+----------+-----+\n",
            "|               1|        1|         1|         1|   60|2022-05-12|   60|\n",
            "|               2|        1|         2|         2|  200|2022-05-12|  400|\n",
            "|               3|        1|         3|         3|  400|2022-05-12| 1200|\n",
            "|               4|        1|         4|         3|  400|2022-05-12| 1200|\n",
            "|               5|        1|         5|         1|  350|2022-05-12|  350|\n",
            "|               6|        1|         6|         2|  800|2022-05-12| 1600|\n",
            "|               7|        2|         1|         1|   60|2022-05-12|   60|\n",
            "|               8|        2|         2|         2|  200|2022-05-12|  400|\n",
            "|               9|        2|         3|         3|  400|2022-05-12| 1200|\n",
            "|              10|        2|         4|         3|  400|2022-05-12| 1200|\n",
            "|              11|        2|         5|         1|  350|2022-05-12|  350|\n",
            "|              12|        3|         6|         2|  800|2022-05-12| 1600|\n",
            "|              13|        3|         1|         1|   60|2022-05-12|   60|\n",
            "|              14|        3|         2|         2|  200|2022-05-12|  400|\n",
            "|              15|        4|         3|         3|  400|2022-05-12| 1200|\n",
            "|              16|        4|         4|         3|  400|2022-05-12| 1200|\n",
            "|              17|        5|         5|         1|  350|2022-05-12|  350|\n",
            "|              18|        6|         6|         2|  800|2022-05-12| 1600|\n",
            "|              19|        7|         1|         1|   60|2022-05-12|   60|\n",
            "|              20|        8|         2|         2|  200|2022-05-12|  400|\n",
            "+----------------+---------+----------+----------+-----+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> 6 - Gravar no formato parquet apenas os pedidos com a VALOR maior que 100</h3>"
      ],
      "metadata": {
        "id": "UBiF6kP0rImz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDcw0v6sy98r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f4cc6b-d04b-4c76-ec08-8eb75b3584c9"
      },
      "source": [
        "#Filtrando os dados com o Where\n",
        "df_valor = df_spark.where(df_spark.valor > 100)\n",
        "df_valor.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---------+----------+----------+-----+----------+-----+\n",
            "|id_venda_detalhe|id_pedido|id_produto|quantidade|valor|data_atual|total|\n",
            "+----------------+---------+----------+----------+-----+----------+-----+\n",
            "|               2|        1|         2|         2|  200|2022-05-12|  400|\n",
            "|               3|        1|         3|         3|  400|2022-05-12| 1200|\n",
            "|               4|        1|         4|         3|  400|2022-05-12| 1200|\n",
            "|               5|        1|         5|         1|  350|2022-05-12|  350|\n",
            "|               6|        1|         6|         2|  800|2022-05-12| 1600|\n",
            "|               8|        2|         2|         2|  200|2022-05-12|  400|\n",
            "|               9|        2|         3|         3|  400|2022-05-12| 1200|\n",
            "|              10|        2|         4|         3|  400|2022-05-12| 1200|\n",
            "|              11|        2|         5|         1|  350|2022-05-12|  350|\n",
            "|              12|        3|         6|         2|  800|2022-05-12| 1600|\n",
            "|              14|        3|         2|         2|  200|2022-05-12|  400|\n",
            "|              15|        4|         3|         3|  400|2022-05-12| 1200|\n",
            "|              16|        4|         4|         3|  400|2022-05-12| 1200|\n",
            "|              17|        5|         5|         1|  350|2022-05-12|  350|\n",
            "|              18|        6|         6|         2|  800|2022-05-12| 1600|\n",
            "|              20|        8|         2|         2|  200|2022-05-12|  400|\n",
            "|              21|        9|         3|         3|  400|2022-05-12| 1200|\n",
            "|              22|       10|         4|         3|  400|2022-05-12| 1200|\n",
            "|              23|       11|         5|         1|  350|2022-05-12|  350|\n",
            "|              24|       11|         6|         2|  800|2022-05-12| 1600|\n",
            "+----------------+---------+----------+----------+-----+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Partition by irá criar uma arvore de pastas\n",
        "df_valor.write.partitionBy(\"id_venda_detalhe\",\"id_pedido\",\"valor\").parquet(\"/content/vendas_spark_/pedidos_partition\")"
      ],
      "metadata": {
        "id": "BwJbaowfs1gR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqok_TEVtOYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989aa4b3-ef63-4724-fc82-d6b0a9089ecc"
      },
      "source": [
        "# Lendo parquet\n",
        "df_p = spark.read.parquet(\"/content/vendas_spark_/pedidos_partition\")\n",
        "df_p.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+-----+----------------+---------+-----+\n",
            "|id_produto|quantidade|data_atual|total|id_venda_detalhe|id_pedido|valor|\n",
            "+----------+----------+----------+-----+----------------+---------+-----+\n",
            "|         5|         1|2022-05-12|  350|              41|       19|  350|\n",
            "|         6|         2|2022-05-12| 1600|              36|       14|  800|\n",
            "|         5|         1|2022-05-12|  350|              60|       29|  350|\n",
            "|         2|         2|2022-05-12|  400|              32|       11|  200|\n",
            "|         4|         3|2022-05-12| 1200|              10|        2|  400|\n",
            "|         2|         2|2022-05-12|  400|              63|       32|  200|\n",
            "|         5|         1|2022-05-12|  350|              11|        2|  350|\n",
            "|         2|         2|2022-05-12|  400|              74|       38|  200|\n",
            "|         6|         2|2022-05-12| 1600|              90|       49|  800|\n",
            "|         2|         2|2022-05-12|  400|              57|       26|  200|\n",
            "|         4|         3|2022-05-12| 1200|              16|        4|  400|\n",
            "|         6|         2|2022-05-12| 1600|              84|       43|  800|\n",
            "|         6|         2|2022-05-12| 1600|              78|       41|  800|\n",
            "|         2|         2|2022-05-12|  400|              26|       11|  200|\n",
            "|         3|         3|2022-05-12| 1200|              15|        4|  400|\n",
            "|         6|         2|2022-05-12| 1600|              24|       11|  800|\n",
            "|         3|         3|2022-05-12| 1200|               3|        1|  400|\n",
            "|         3|         3|2022-05-12| 1200|              27|       11|  400|\n",
            "|         5|         1|2022-05-12|  350|               5|        1|  350|\n",
            "|         2|         2|2022-05-12|  400|               2|        1|  200|\n",
            "+----------+----------+----------+-----+----------------+---------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
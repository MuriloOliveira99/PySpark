{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# O que é Streaming?\n",
        "  - processamento de dados contínuo\n",
        "  - Em tempo real, ou próximo a tempo real"
      ],
      "metadata": {
        "id": "9wQ-pqSqsBFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Por que Streaming é importante?\n",
        "  - Analisar dados é um processo de transformação:\n",
        "    - Dados -> processamento -> informação\n",
        "  - A informação tem um valor\n",
        "  - O valor de qualquer informação está relacionada diretamente ao tempo!"
      ],
      "metadata": {
        "id": "VwX4cISVtdnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming\n",
        "  - Contínuo - sem fim\n",
        "  - Carregado a medida que é produzido\n",
        "  - Processado a medida que é produzido\n",
        "\n",
        "# Batch\n",
        "  - Com inicio e fim\n",
        "  - Carregamento em lote\n",
        "  - Processamento em lote\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zFXtC-JgtHkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Como funciona o processo de Streaming no Spark?\n",
        "  - Micro-Batchs\n",
        "    - Bloco de dados produzidos em intervalo de tempo\n",
        "  - Structured Streaming\n",
        "    - Segunda geração de processamento de streaming de Spark (Dstream foi a primeira)\n",
        "    - Garantia de processamento único de cada registro (end-to-end exactly-once guarantees)\n",
        "\n",
        "# Modo de Saída\n",
        "  - APPEND: Só novas linhas. Suporta apenas consultas stateless (não depende da informação dos registros anteriores)\n",
        "  - UPDATE: apenas linhas que foram atualizadas\n",
        "  - COMPLETE: toda a tabela é atualizada\n",
        "\n",
        "# TRIGGER (Como o processo de monitoramento vai funcionar?)\n",
        "  - Formas:\n",
        "    - Default: dispara quando o micro batch termina\n",
        "    - Tempo\n",
        "    - Once: apenas uma única vez\n",
        "    - Continuous: processamento contínuo\n",
        "\n",
        "  - Parar o processo\n",
        "    - stop()"
      ],
      "metadata": {
        "id": "ScyNDIcxuD1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpointdir\n",
        " - Diretório onde o estado de andamento é salvo\n",
        " - Se você parar o prcoesso e reiniciar com o mesmo diretório, ele segue de onde parou\n",
        "\n",
        "# Métodos semelhantes os de batch\n",
        " - readstream em vez de read\n",
        " - writestream em vez de write\n",
        "\n",
        "# Source e Sinks (origem e destino) que não tem suporte\n",
        "  - Métodos de batch podem ser usados (read, write):\n",
        "    - foreachbatch: opera no micro-batch\n",
        "    - Foreach: opera a cada linha\n",
        "  - Algumas garantias são perdidas: por exemplo, exactly-once::\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Io44Lxfjv8wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "pip install spark\n",
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhI3cKWOx8aH",
        "outputId": "dea9a4e7-3d43-4d54-bc48-c18b9115da38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spark\n",
            "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
            "Building wheels for collected packages: spark\n",
            "  Building wheel for spark (setup.py): started\n",
            "  Building wheel for spark (setup.py): finished with status 'done'\n",
            "  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58762 sha256=dc402f0205a0765351a9f1d4043fedf27881daaf5134e7105131528aca3e0544\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/0e/f1/164619f9920fb447d294afaae11a7715bd442ded7225953d72\n",
            "Successfully built spark\n",
            "Installing collected packages: spark\n",
            "Successfully installed spark-0.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py): started\n",
            "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=4ef565eb31d447e43a74a2b4e26baf3bec6f2e11b1aad80a1f11dc6b14a5e64a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "1GS005DexlDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### De JSON para o postgreSQL"
      ],
      "metadata": {
        "id": "AV_3IEbL4OWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obs: ao executar o código, lembre-se de adicionar um arquivo .json na pasta teststreaming\n",
        "# estrutura pastas: https://prnt.sc/nFgt2LDD4Yeu\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  spark = SparkSession.builder.appName('Streaming').getOrCreate()\n",
        "\n",
        "  # Definindo um schema pro json\n",
        "  jsonschema = \"\"\"nome STRING, \n",
        "                postagem STRING, \n",
        "                data INT\"\"\"\n",
        "\n",
        "# lê todos os arquivos .json que estivar na pasta teststreaming\n",
        "df = spark.readStream.json('/content/murilo/teststreaming', schema=jsonschema)\n",
        "\n",
        "# guardar o estado da sessão da aplicacao streaming na pasta temp\n",
        "# de cada arquivo .json inserido em teststreaming\n",
        "diretorio = '/content/murilo/temp'\n",
        "\n",
        "def atualiza_postgresql(data_f, batch_id):\n",
        "  data_f.write.format('jdbc').option('url', 'jdbc:postgresql://localhost:5432/posts')\\\n",
        "                             .option('dbtable', 'posts')\\\n",
        "                             .option('user', 'postgres')\\\n",
        "                             .option('password', '123456')\\\n",
        "                             .option('driver', 'org.postgresql.Driver')\\\n",
        "                             .mode('append').save()\n",
        "              \n",
        "\n",
        "stcal = df.writeStream.foreachBatch(atualiza_postgresql)\\\n",
        "                      .outputMode('append')\\\n",
        "                      .trigger(processingTime = '5 second')\\\n",
        "                      .option('checkpointlocation', diretorio)\\\n",
        "                      .start()\n",
        "\n",
        "# espera o fim do processo de alguma forma\n",
        "stcal.awaitTermination()"
      ],
      "metadata": {
        "id": "HqV-3p-Hxor4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}